# -*- coding: utf-8 -*-
"""TF2FeedForward.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UltFBhg9JQZFT2vMQjCh0XEMKuLzOdpU
"""

import tensorflow as tf

class TF2FeedForward:
  def __init__(self):
    self.w1 = None
    self.w2 = None
    self.w3 = None
    self.w4 = None
    self.b1 = None
    self.b2 = None
    self.b3 = None
    self.b4 = None
    self.hidden1 = 256
    self.hidden2 = 256
    self.hidden3 = 256
    self.input_layer = 784
    self.output = 10
    self.optimizer = tf.optimizers.Adam(0.01)
    
  def initialize_params(self):
    self.b1 = tf.Variable(tf.random.normal([self.hidden1]))
    self.b2 = tf.Variable(tf.random.normal([self.hidden2]))
    self.b3 = tf.Variable(tf.random.normal([self.hidden3]))
    self.b4 = tf.Variable(tf.random.normal([self.output]))

    self.w1 = tf.Variable(tf.random.normal([self.input_layer, self.hidden1]))
    self.w2 = tf.Variable(tf.random.normal([self.hidden1, self.hidden2]))
    self.w3 = tf.Variable(tf.random.normal([self.hidden2, self.hidden3]))
    self.w4 = tf.Variable(tf.random.normal([self.hidden3, self.output]))
    
  def predict(self, input_logits):
    input_logits = tf.cast(input_logits, tf.float32)
    
    layer_1 = tf.nn.relu(tf.add(tf.matmul(input_logits, self.w1), self.b1))
    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, self.w2), self.b2))
    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, self.w3), self.b3))
    
    out_layer = tf.add(tf.matmul(layer_3, self.w4), self.b4)

    return out_layer
  
  def loss(self, logits, labels):
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))
  
  def train_step(self, input_d, labels):                                                                                                                                                                                           
    with tf.GradientTape() as tape:
      logits = self.predict(input_d)
      loss = self.loss(logits, labels)
      
    gradients = tape.gradient(logits, [self.w1, self.w2, self.w3, self.w4, self.b1, self.b2, self.b3, self.b4])
    self.optimizer.apply_gradients(zip(gradients, [self.w1, self.w2, self.w3, self.w4, self.b1, self.b2, self.b3, self.b4]))
    
    return loss