# -*- coding: utf-8 -*-
"""tf2logisticregression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FaaiqYFfKObM15BCSzYsR4c2s8HDueMU
"""

import tensorflow as tf

class TF2LogisticRegression():
  def __init__(self):
    self.W = None
    self.b = None
    self.logit = None
    self.optimizer = tf.optimizers.Adam(0.01)
    self.number_of_features = 784
    self.number_of_classes = 10

  def initialize_params(self):
    self.W = tf.Variable(tf.random.normal(shape=[self.number_of_features, 1]))
    self.b = tf.Variable(tf.random.normal(shape=[1, self.number_of_classes]))
   
  def loss(self, logits, labels):
    
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))
    
  def predict(self, batch_features):
    batch_features = tf.cast(batch_features, tf.float32)
    logit = tf.matmul(batch_features, self.W) + self.b
    
    return logit
    
  def train_step(self, batch_features, batch_labels):
    with tf.GradientTape() as tape:
      logits = self.predict(batch_features)
      logits = tf.sigmoid(logits)
      loss = self.loss(logits, batch_labels)
        
    gradients = tape.gradient(logits, [self.W])
    
    self.optimizer.apply_gradients(zip(gradients, [self.W]))
    
    return loss